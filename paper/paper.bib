% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@article{chomsky1956three,
  title={Three models for the description of language},
  author={Chomsky, Noam},
  journal={IRE Transactions on information theory},
  volume={2},
  number={3},
  pages={113--124},
  year={1956},
  publisher={IEEE},
  doi={10.1109/TIT.1956.1056813}
}

@article{miller2003cognitive,
  title={The cognitive revolution: a historical perspective},
  author={Miller, George A},
  journal={Trends in cognitive sciences},
  volume={7},
  number={3},
  pages={141--144},
  year={2003},
  publisher={Elsevier},
  doi={10.1016/S1364-6613(03)00029-9}
}

@article{graves2014neural,
  title={Neural turing machines},
  author={Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  journal={arXiv preprint arXiv:1410.5401},
  year={2014},
  doi={10.48550/arXiv.1410.5401}
}

@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014},
  doi={10.48550/arXiv.1409.0473}
}

@inproceedings{iyer2016summarizing,
  title={Summarizing source code using a neural attention model},
  author={Iyer, Srinivasan and Konstas, Ioannis and Cheung, Alvin and Zettlemoyer, Luke},
  booktitle={54th Annual Meeting of the Association for Computational Linguistics 2016},
  pages={2073--2083},
  year={2016},
  organization={Association for Computational Linguistics},
  doi={10.18653/v1/P16-1208}
}

@article{ling2016latent,
  title={Latent predictor networks for code generation},
  author={Ling, Wang and Grefenstette, Edward and Hermann, Karl Moritz and Ko{\v{c}}isk{\`y}, Tom{\'a}{\v{s}} and Senior, Andrew and Wang, Fumin and Blunsom, Phil},
  journal={arXiv preprint arXiv:1603.06744},
  year={2016},
  doi={10.48550/arXiv.1603.06744}
}

@article{yin2017syntactic,
  title={A syntactic neural model for general-purpose code generation},
  author={Yin, Pengcheng and Neubig, Graham},
  journal={arXiv preprint arXiv:1704.01696},
  year={2017},
  doi={10.48550/arXiv.1704.01696}
}

@inproceedings{allamanis2013mining,
  title={Mining source code repositories at massive scale using language modeling},
  author={Allamanis, Miltiadis and Sutton, Charles},
  booktitle={2013 10th working conference on mining software repositories (MSR)},
  pages={207--216},
  year={2013},
  organization={IEEE},
  doi={10.1109/MSR.2013.6624030}
}

@article{bhoopchand2016learning,
  title={Learning python code suggestion with a sparse pointer network},
  author={Bhoopchand, Avishkar and Rockt{\"a}schel, Tim and Barr, Earl and Riedel, Sebastian},
  journal={arXiv preprint arXiv:1611.08307},
  year={2016},
  doi={10.48550/arXiv.1611.08307}
}

@inproceedings{oda2015learning,
  title={Learning to generate pseudo-code from source code using statistical machine translation},
  author={Oda, Yusuke and Fudaba, Hiroyuki and Neubig, Graham and Hata, Hideaki and Sakti, Sakriani and Toda, Tomoki and Nakamura, Satoshi},
  booktitle={2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE)},
  pages={574--584},
  year={2015},
  organization={IEEE},
  doi={10.1109/ASE.2015.72}
}

@inproceedings{quirk2015language,
  title={Language to code: Learning semantic parsers for if-this-then-that recipes},
  author={Quirk, Chris and Mooney, Raymond and Galley, Michel},
  booktitle={Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={878--888},
  year={2015},
  doi={10.3115/v1/P15-1085}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020},
  doi={10.48550/arXiv.2005.14165}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022},
  doi={10.48550/arXiv.2203.02155}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017},
  doi={10.48550/arXiv.1706.03762}
}

@article{lester2021power,
  title={The power of scale for parameter-efficient prompt tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  journal={arXiv preprint arXiv:2104.08691},
  year={2021},
  doi={10.48550/arXiv.2104.08691}
}

@misc{gpt-3.5-turbo,
  author = {OpenAI},
  title = {gpt-3.5-turbo},
  year = {2023},
  howpublished = {\url{https://github.com/openai/gpt-3.5-turbo}},
  note = {Accessed: 2024-06-08}
}

@misc{gpt-4,
  author = {OpenAI},
  title = {gpt-4},
  year = {2023},
  howpublished = {\url{https://github.com/openai/gpt-4}},
  note = {Accessed: 2024-06-08}
}

@misc{gpt-4-32k,
  author = {OpenAI},
  title = {gpt-4-32k},
  year = {2023},
  howpublished = {\url{https://github.com/openai/gpt-4-32k}},
  note = {Accessed: 2024-06-08}
}

@misc{llama-2-7b-chat-gptq,
  author = {TheBloke},
  title = {Llama-2-7B-Chat-GPTQ},
  year = {2023},
  howpublished = {\url{https://github.com/TheBloke/Llama-2-7B-Chat-GPTQ}},
  note = {Accessed: 2024-06-08}
}

@misc{code-llama-7b-instruct-gptq,
  author = {TheBloke},
  title = {CodeLlama-7B-Instruct-GPTQ},
  year = {2023},
  howpublished = {\url{https://github.com/TheBloke/CodeLlama-7B-Instruct-GPTQ}},
  note = {Accessed: 2024-06-08}
}

@misc{llama-2-7b-chat-hf,
  author = {Meta},
  title = {Llama-2-7b-chat-hf},
  year = {2023},
  howpublished = {\url{https://github.com/meta-llama/Llama-2-7b-chat-hf}},
  note = {Accessed: 2024-06-08}
}

@misc{code-llama-7b-instruct-hf,
  author = {Meta},
  title = {CodeLlama-7b-Instruct-hf},
  year = {2023},
  howpublished = {\url{https://github.com/meta-llama/CodeLlama-7b-Instruct-hf}},
  note = {Accessed: 2024-06-08}
}

@misc{gemma-2b-it,
  author = {Google},
  title = {gemma-2b-it},
  year = {2023},
  howpublished = {\url{https://github.com/google/gemma-2b-it}},
  note = {Accessed: 2024-06-08}
}

@misc{codegemma-2b-it,
  author = {Google},
  title = {codegemma-2b-it},
  year = {2023},
  howpublished = {\url{https://github.com/google/codegemma-2b-it}},
  note = {Accessed: 2024-06-08}
}

@misc{autodoc-chatgpt,
  author = {Awekrx},
  title = {AutoDoc-ChatGPT},
  year = {2023},
  howpublished = {\url{https://github.com/awekrx/AutoDoc-ChatGPT}},
  note = {Accessed: 2024-06-08}
}

@misc{context-labs-autodoc,
  author = {Context Labs},
  title = {AutoDoc},
  year = {2023},
  howpublished = {\url{https://github.com/context-labs/autodoc}},
  note = {Accessed: 2024-06-08}
}

@misc{microsoft-auto-github-docs-generator,
  author = {Microsoft},
  title = {Auto-GitHub-Docs-Generator},
  year = {2023},
  howpublished = {\url{https://github.com/microsoft/auto-github-docs-generator}},
  note = {Accessed: 2024-06-08}
}

@misc{sentence-transformers-all-mpnet-base-v2,
  author = {HuggingFace},
  title = {Sentence Transformers: all-mpnet-base-v2},
  year = {2023},
  howpublished = {\url{https://huggingface.co/sentence-transformers/all-mpnet-base-v2}},
  note = {Accessed: 2024-06-08}
}

@article{barone2017parallel,
  title={A parallel corpus of python functions and documentation strings for automated code documentation and code generation},
  author={Barone, Antonio Valerio Miceli and Sennrich, Rico},
  journal={arXiv preprint arXiv:1707.02275},
  year={2017},
  doi={10.48550/arXiv.1707.02275}
}

@article{malkov2018efficient,
  title={Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs},
  author={Malkov, Yu A and Yashunin, Dmitry A},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={42},
  number={4},
  pages={824--836},
  year={2018},
  publisher={IEEE},
  doi={10.1109/TPAMI.2018.2889473}
}

@article{dettmers2023qlora,
  title={QLoRA: Efficient Finetuning of Quantized LLMs},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2305.14314},
  year={2023},
  doi={10.48550/arXiv.2305.14314}
}

@inproceedings{
hu2022lora,
title={Lo{RA}: Low-Rank Adaptation of Large Language Models},
author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=nZeVKeeFYf9},
doi={10.48550/arXiv.2106.09685}
}

@article{zhang2023dynamically,
  title={SockDef: A Dynamically Adaptive Defense to a Novel Attack on Review Fraud Detection Engines},
  author={Zhang, Youzhi and Chakrabarty, Sayak and Liu, Rui and Pugliese, Andrea and Subrahmanian, VS},
  journal={IEEE Transactions on Computational Social Systems},
  year={2023},
  publisher={IEEE},
  doi={10.1109/TCSS.2023.3321345}
}

@article{makarychev2024single,
  title={Single-Pass Pivot Algorithm for Correlation Clustering. Keep it simple!},
  author={Makarychev, Konstantin and Chakrabarty, Sayak},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024},
  doi={10.48550/arXiv.2305.13560}
}

@article{datta2024consistency,
  title={On the consistency of maximum likelihood estimation of probabilistic principal component analysis},
  author={Datta, Arghya and Chakrabarty, Sayak},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024},
  doi={10.48550/arXiv.2311.05046}
}
