{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning with Llama 2, Bits and Bytes, and QLoRA\n",
    "\n",
    "Today we'll explore fine-tuning the Llama 2 model available using QLoRA, Bits and Bytes, and PEFT.\n",
    "\n",
    "- QLoRA: [Quantized Low Rank Adapters](https://arxiv.org/pdf/2305.14314.pdf) - this is a method for fine-tuning LLMs that uses a small number of quantized, updateable parameters to limit the complexity of training. This technique also allows those small sets of parameters to be added efficiently into the model itself, which means you can do fine-tuning on lots of data sets, potentially, and swap these \"adapters\" into your model when necessary.\n",
    "- [Bits and Bytes](https://github.com/TimDettmers/bitsandbytes): An excellent package by Tim Dettmers et al., which provides a lightweight wrapper around custom CUDA functions that make LLMs go faster - optimizers, matrix mults, and quantization. In this notebook we'll be using the library to load our model as efficiently as possible.\n",
    "- [PEFT](https://github.com/huggingface/peft): An excellent Huggingface library that enables a number Parameter Efficient Fine-tuning (PEFT) methods, which again make it less expensive to fine-tune LLMs - especially on more lightweight hardware like that present in Kaggle notebooks.\n",
    "\n",
    "Many thanks to [Bojan Tunguz](https://www.kaggle.com/tunguz) for his excellent [Jeopardy dataset](https://www.kaggle.com/datasets/tunguz/200000-jeopardy-questions)!\n",
    "\n",
    "This notebook is based on [an excellent example from LangChain](https://github.com/asokraju/LangChainDatasetForge/blob/main/Finetuning_Falcon_7b.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wIa8WRIHvuZy"
   },
   "source": [
    "## Package Installation\n",
    "\n",
    "Note that we're loading very specific versions of these libraries. Dependencies in this space can be quite difficult to untangle, and simply taking the latest version of each library can lead to conflicting version requirements. It's a good idea to take note of which versions work for your particular use case, and `pip install` them directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-05-23T12:21:58.849851Z",
     "iopub.status.busy": "2024-05-23T12:21:58.849106Z",
     "iopub.status.idle": "2024-05-23T12:24:42.714261Z",
     "shell.execute_reply": "2024-05-23T12:24:42.713141Z",
     "shell.execute_reply.started": "2024-05-23T12:21:58.849820Z"
    },
    "id": "g3agEMJEnKsl",
    "outputId": "56ab843a-e9c8-4158-8c04-18d77aec44fd",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%pip install -qqq bitsandbytes\n",
    "%pip install -qqq torch\n",
    "%pip install -qqq -U git+https://github.com/huggingface/transformers.git\n",
    "%pip install -qqq -U git+https://github.com/huggingface/peft.git\n",
    "%pip install -qqq -U git+https://github.com/huggingface/accelerate.git\n",
    "%pip install -qqq datasets\n",
    "%pip install -qqq loralib\n",
    "%pip install -qqq einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T11:09:09.148558Z",
     "iopub.status.busy": "2024-05-21T11:09:09.147392Z",
     "iopub.status.idle": "2024-05-21T11:09:30.589305Z",
     "shell.execute_reply": "2024-05-21T11:09:30.588042Z",
     "shell.execute_reply.started": "2024-05-21T11:09:09.148520Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%pip install -q optimum auto-gptq pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T12:24:42.716682Z",
     "iopub.status.busy": "2024-05-23T12:24:42.716361Z",
     "iopub.status.idle": "2024-05-23T12:24:48.696015Z",
     "shell.execute_reply": "2024-05-23T12:24:48.695079Z",
     "shell.execute_reply.started": "2024-05-23T12:24:42.716652Z"
    },
    "id": "dv3aJo8Anhyw",
    "outputId": "66f7b274-28a9-45b4-c0e6-d25194424594",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import load_dataset, Dataset\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "from peft import LoraConfig, PeftConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AgqJriqjwMyK"
   },
   "source": [
    "# Loading and preparing our model\n",
    "\n",
    "We're going to use the Llama 2 7B model for our test. We'll be using Bits and Bytes to load it in 4-bit format, which should reduce memory consumption considerably, at a cost of some accuracy.\n",
    "\n",
    "Note the parameters in `BitsAndBytesConfig` - this is a fairly standard 4-bit quantization configuration, loading the weights in 4-bit format, using a straightforward format (`normal float 4`) with double quantization to improve QLoRA's resolution. The weights are converted back to `bfloat16` for weight updates, then the extra precision is discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mllA2Ka_ol13",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model = \"/kaggle/input/llama-2/pytorch/7b-chat-hf/1\"\n",
    "# MODEL_NAME = model\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     MODEL_NAME,\n",
    "#     device_map=\"auto\",\n",
    "#     trust_remote_code=True,\n",
    "#     quantization_config=bnb_config\n",
    "# )\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T12:24:48.697613Z",
     "iopub.status.busy": "2024-05-23T12:24:48.697147Z",
     "iopub.status.idle": "2024-05-23T12:25:25.163356Z",
     "shell.execute_reply": "2024-05-23T12:25:25.162575Z",
     "shell.execute_reply.started": "2024-05-23T12:24:48.697586Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model = \"google/gemma-2b-it\"\n",
    "# MODEL_NAME = model\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     MODEL_NAME,\n",
    "#     device_map=\"auto\",\n",
    "#     trust_remote_code=True,\n",
    "#     quantization_config=bnb_config,\n",
    "#     token='<token>'\n",
    "# )\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token='<token>')\n",
    "# tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we'll use a nice PEFT wrapper to set up our model for training / fine-tuning. Specifically this function sets the output embedding layer to allow gradient updates, as well as performing some type casting on various components to ensure the model is ready to be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T12:25:25.165686Z",
     "iopub.status.busy": "2024-05-23T12:25:25.165377Z",
     "iopub.status.idle": "2024-05-23T12:25:25.184291Z",
     "shell.execute_reply": "2024-05-23T12:25:25.183565Z",
     "shell.execute_reply.started": "2024-05-23T12:25:25.165661Z"
    },
    "id": "na8DUq4IoqpB",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we define some helper functions - their purpose is to properly identify our update layers so we can... update them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T12:25:25.185688Z",
     "iopub.status.busy": "2024-05-23T12:25:25.185329Z",
     "iopub.status.idle": "2024-05-23T12:25:25.193947Z",
     "shell.execute_reply": "2024-05-23T12:25:25.193083Z",
     "shell.execute_reply.started": "2024-05-23T12:25:25.185654Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T12:25:25.195344Z",
     "iopub.status.busy": "2024-05-23T12:25:25.195023Z",
     "iopub.status.idle": "2024-05-23T12:25:25.203441Z",
     "shell.execute_reply": "2024-05-23T12:25:25.202469Z",
     "shell.execute_reply.started": "2024-05-23T12:25:25.195319Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def get_num_layers(model):\n",
    "    numbers = set()\n",
    "    for name, _ in model.named_parameters():\n",
    "        for number in re.findall(r'\\d+', name):\n",
    "            numbers.add(int(number))\n",
    "    print(max(numbers))\n",
    "    return max(numbers)\n",
    "\n",
    "def get_last_layer_linears(model):\n",
    "    names = []\n",
    "    \n",
    "    num_layers = get_num_layers(model)\n",
    "    for name, module in model.named_modules():\n",
    "        if str(num_layers) in name and not \"encoder\" in name:\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                names.append(name)\n",
    "    print(names)\n",
    "    return names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LORA config\n",
    "\n",
    "Some key elements from this configuration:\n",
    "1. `r` is the width of the small update layer. In theory, this should be set wide enough to capture the complexity of the problem you're attempting to fine-tune for. More simple problems may be able to get away with smaller `r`. In our case, we'll go very small, largely for the sake of speed.\n",
    "2. `target_modules` is set using our helper functions - every layer identified by that function will be included in the PEFT update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T12:25:25.205599Z",
     "iopub.status.busy": "2024-05-23T12:25:25.205231Z",
     "iopub.status.idle": "2024-05-23T12:25:25.236478Z",
     "shell.execute_reply": "2024-05-23T12:25:25.235650Z",
     "shell.execute_reply.started": "2024-05-23T12:25:25.205568Z"
    },
    "id": "C4Qk3fGLoraw",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=2,\n",
    "    lora_alpha=32,\n",
    "    target_modules=get_last_layer_linears(model),\n",
    "    # target_modules = [\"k_proj\",\"o_proj\",\"q_proj\",\"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load some data\n",
    "\n",
    "Here, we're loading a 200,000 question Jeopardy dataset. In the interests of time we won't load all of them - just the first 1000 - but we'll fine-tune our model using the question and answers. Note that what we're training the model to do is use its existing knowledge (plus whatever little it learns from our question-answer pairs) to answer questions in the *format* we want, specifically short answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T12:25:25.237679Z",
     "iopub.status.busy": "2024-05-23T12:25:25.237419Z",
     "iopub.status.idle": "2024-05-23T12:25:25.298238Z",
     "shell.execute_reply": "2024-05-23T12:25:25.297528Z",
     "shell.execute_reply.started": "2024-05-23T12:25:25.237656Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"readme_qa.csv\") # Use readme_qa.csv generated from data.ipynb\n",
    "df.columns = [str(q).strip() for q in df.columns]\n",
    "\n",
    "data = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T06:36:39.569842Z",
     "iopub.status.busy": "2024-05-08T06:36:39.569492Z",
     "iopub.status.idle": "2024-05-08T06:36:39.577848Z",
     "shell.execute_reply": "2024-05-08T06:36:39.576834Z",
     "shell.execute_reply.started": "2024-05-08T06:36:39.569808Z"
    },
    "id": "_Pb9RA5NovNS",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# prompt = df[\"Question\"].values[0] + \". Answer as briefly as possible: \".strip()\n",
    "# prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T12:26:49.814219Z",
     "iopub.status.busy": "2024-05-23T12:26:49.813856Z",
     "iopub.status.idle": "2024-05-23T12:26:49.824269Z",
     "shell.execute_reply": "2024-05-23T12:26:49.823392Z",
     "shell.execute_reply.started": "2024-05-23T12:26:49.814190Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "project_name = df[\"Repo\"].values[10820]\n",
    "repository_url = df[\"Repo Url\"].values[10820]\n",
    "target_audience = \"smart developer\"\n",
    "question = df[\"Question\"].values[10820]\n",
    "context = df[\"Context\"].values[10820]\n",
    "content_type = \"docs\"\n",
    "prompt = f\"\"\"You are an AI assistant for a software project called {project_name}. You are trained on all the {content_type} that makes up this project.\n",
    "    The {content_type} for the project is located at {repository_url}.\n",
    "    You are given a repository which might contain several modules and each module will contain a set of files.\n",
    "    Look at the source code in the repository and you have to generate content for the section of a README.md file following the heading given below. If you use any hyperlinks, they should link back to the github repository shared with you.\n",
    "    You should only use hyperlinks that are explicitly listed in the context. Do NOT make up a hyperlink that is not listed.\n",
    "\n",
    "    Assume the reader is a {target_audience} but is not deeply familiar with {project_name}.\n",
    "    Assume the reader does not know anything about how the project is structured or which folders/files do what and what functions are written in which files and what these functions do.\n",
    "    If you don't know how to fill up the readme.md file in one of its sections, leave that part blank. Don't try to make up any content.\n",
    "    Do not include information that is not directly relevant to repository, even though the names of the functions might be common or is frequently used in several other places.\n",
    "    Keep your response between 100 and 300 words. DO NOT RETURN MORE THAN 300 WORDS. Provide the answer in correct markdown format.\n",
    "\n",
    "    Question: {question}\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Answer in Markdown:\"\"\"\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHYgWlyvwy4E"
   },
   "source": [
    "## Let's generate!\n",
    "\n",
    "Below we're setting up our generative model:\n",
    "- Top P: a method for choosing from among a selection of most probable outputs, as opposed to greedily just taking the highest)\n",
    "- Temperature: a modulation on the softmax function used to determine the values of our outputs\n",
    "- We limit the return sequences to 1 - only one answer is allowed! - and deliberately force the answer to be short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T12:25:25.363470Z",
     "iopub.status.busy": "2024-05-23T12:25:25.363233Z",
     "iopub.status.idle": "2024-05-23T12:25:25.371384Z",
     "shell.execute_reply": "2024-05-23T12:25:25.370516Z",
     "shell.execute_reply.started": "2024-05-23T12:25:25.363443Z"
    },
    "id": "YiqCdCD2oyPH",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 512\n",
    "generation_config.temperature = 0.7\n",
    "generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.repetition_penalty = 1.1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll generate an answer to our first question, just to see how the model does!\n",
    "\n",
    "It's fascinatingly wrong. :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T12:06:55.308570Z",
     "iopub.status.busy": "2024-05-23T12:06:55.308190Z",
     "iopub.status.idle": "2024-05-23T12:07:08.421661Z",
     "shell.execute_reply": "2024-05-23T12:07:08.420686Z",
     "shell.execute_reply.started": "2024-05-23T12:06:55.308539Z"
    },
    "id": "o2ELFG0no1xR",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "device = \"cuda\"\n",
    "\n",
    "encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids = encoding.input_ids,\n",
    "        attention_mask = encoding.attention_mask,\n",
    "        generation_config = generation_config\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QAe7n7T4jP-D"
   },
   "source": [
    "## Format our fine-tuning data\n",
    "\n",
    "We'll match the prompt setup we used above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-23T12:26:55.514848Z",
     "iopub.status.idle": "2024-05-23T12:26:56.254319Z",
     "shell.execute_reply": "2024-05-23T12:26:56.253352Z",
     "shell.execute_reply.started": "2024-05-23T12:26:55.515569Z"
    },
    "id": "lm60o2_No7Jz",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "#     return f\"\"\"\n",
    "#             {data_point[\"Question\"]}. \n",
    "#             Answer as briefly as possible: {data_point[\"Answer\"]}\n",
    "#             \"\"\".strip()\n",
    "    return f\"\"\"You are an AI assistant for a software project called {data_point[\"Repo\"]}. You are trained on all the {content_type} that makes up this project.\n",
    "    The docs for the project is located at {data_point[\"Repo Url\"]}.\n",
    "    You are given a repository which might contain several modules and each module will contain a set of files.\n",
    "    Look at the source code in the repository and you have to generate content for the section of a README.md file following the heading given below. If you use any hyperlinks, they should link back to the github repository shared with you.\n",
    "    You should only use hyperlinks that are explicitly listed in the context. Do NOT make up a hyperlink that is not listed.\n",
    "\n",
    "    Assume the reader is a smart developer but is not deeply familiar with {data_point[\"Repo\"]}.\n",
    "    Assume the reader does not know anything about how the project is structured or which folders/files do what and what functions are written in which files and what these functions do.\n",
    "    If you don't know how to fill up the readme.md file in one of its sections, leave that part blank. Don't try to make up any content.\n",
    "    Do not include information that is not directly relevant to repository, even though the names of the functions might be common or is frequently used in several other places.\n",
    "    Keep your response between 100 and 300 words. DO NOT RETURN MORE THAN 300 WORDS. Provide the answer in correct markdown format.\n",
    "\n",
    "    Question: {data_point[\"Question\"]}\n",
    "    Context:\n",
    "    {data_point[\"Context\"]}\n",
    "\n",
    "    Answer in Markdown:\n",
    "    {data_point[\"Answer\"]}\n",
    "    \"\"\"\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = generate_prompt(data_point)\n",
    "    tokenized_full_prompt = tokenizer(full_prompt, padding=True, truncation=True)\n",
    "    return tokenized_full_prompt\n",
    "\n",
    "data = Dataset.from_pandas(df)\n",
    "data = data.shuffle().map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCrTXUqXk0S9"
   },
   "source": [
    "## Train!\n",
    "\n",
    "Now, we'll use our data to update our model. Using the Huggingface `transformers` library, let's set up our training loop and then run it. Note that we are ONLY making one pass on all this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA']=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T12:32:56.159367Z",
     "iopub.status.busy": "2024-05-23T12:32:56.158408Z",
     "iopub.status.idle": "2024-05-23T12:33:03.663596Z",
     "shell.execute_reply": "2024-05-23T12:33:03.662176Z",
     "shell.execute_reply.started": "2024-05-23T12:32:56.159329Z"
    },
    "id": "PGneIe1xpUJV",
    "outputId": "8cdac9ac-d6bf-4d8f-b954-febf7f140591",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=1e-4,\n",
    "    fp16=True,\n",
    "    output_dir=\"outputs\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.01,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=data,\n",
    "    args=training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and using the model later\n",
    "\n",
    "Now, we'll save the PEFT fine-tuned model, then load it and use it to generate some more answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T06:59:35.522808Z",
     "iopub.status.busy": "2024-05-08T06:59:35.522441Z",
     "iopub.status.idle": "2024-05-08T06:59:50.303758Z",
     "shell.execute_reply": "2024-05-08T06:59:50.302828Z",
     "shell.execute_reply.started": "2024-05-08T06:59:35.522781Z"
    },
    "id": "Vmce-aSesAHV",
    "outputId": "4bf93e78-2a0b-404c-8b05-3748db1bdc52",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"outputs/trained-model\")\n",
    "PEFT_MODEL = \"outputs/trained-model\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(PEFT_MODEL)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    return_dict=True,\n",
    "    # quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = PeftModel.from_pretrained(model, PEFT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T07:00:06.408820Z",
     "iopub.status.busy": "2024-05-08T07:00:06.407590Z",
     "iopub.status.idle": "2024-05-08T07:00:06.414259Z",
     "shell.execute_reply": "2024-05-08T07:00:06.413181Z",
     "shell.execute_reply.started": "2024-05-08T07:00:06.408782Z"
    },
    "id": "vgIHyPUasD0b",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 512\n",
    "generation_config.temperature = 0.7\n",
    "generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.repetition_penalty = 1.1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T07:00:10.373393Z",
     "iopub.status.busy": "2024-05-08T07:00:10.373001Z",
     "iopub.status.idle": "2024-05-08T07:00:12.135173Z",
     "shell.execute_reply": "2024-05-08T07:00:12.134130Z",
     "shell.execute_reply.started": "2024-05-08T07:00:10.373362Z"
    },
    "id": "63Zxai-isGhJ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "project_name = df[\"Repo\"].values[10820]\n",
    "repository_url = df[\"Repo Url\"].values[10820]\n",
    "target_audience = \"smart developer\"\n",
    "question = df[\"Question\"].values[10820]\n",
    "context = df[\"Context\"].values[10820]\n",
    "content_type = \"docs\"\n",
    "prompt = f\"\"\"You are an AI assistant for a software project called {project_name}. You are trained on all the {content_type} that makes up this project.\n",
    "    The {content_type} for the project is located at {repository_url}.\n",
    "    You are given a repository which might contain several modules and each module will contain a set of files.\n",
    "    Look at the source code in the repository and you have to generate content for the section of a README.md file following the heading given below. If you use any hyperlinks, they should link back to the github repository shared with you.\n",
    "    You should only use hyperlinks that are explicitly listed in the context. Do NOT make up a hyperlink that is not listed.\n",
    "\n",
    "    Assume the reader is a {target_audience} but is not deeply familiar with {project_name}.\n",
    "    Assume the reader does not know anything about how the project is structured or which folders/files do what and what functions are written in which files and what these functions do.\n",
    "    If you don't know how to fill up the readme.md file in one of its sections, leave that part blank. Don't try to make up any content.\n",
    "    Do not include information that is not directly relevant to repository, even though the names of the functions might be common or is frequently used in several other places.\n",
    "    Keep your response between 100 and 300 words. DO NOT RETURN MORE THAN 300 WORDS. Provide the answer in correct markdown format.\n",
    "\n",
    "    Question: {question}\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Answer in Markdown:\"\"\"\n",
    "    \n",
    "device = \"cuda\"\n",
    "encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.inference_mode():\n",
    "  outputs = model.generate(\n",
    "      input_ids = encoding.input_ids,\n",
    "      attention_mask = encoding.attention_mask,\n",
    "      generation_config = generation_config\n",
    "  )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 412543,
     "sourceId": 789660,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5010308,
     "sourceId": 8417065,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 3093,
     "sourceId": 4298,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 8318,
     "sourceId": 28785,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30559,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
