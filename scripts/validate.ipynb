{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U readme-ready bert_score nltk "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "HF_TOKEN = \"hf_fake_token\" # Replace token\n",
    "OPENAI_API_KEY = \"fake_openai_api_key\" # Replace api key\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "os.environ[\"HF_TOKEN\"] = HF_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import bert_score\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "\n",
    "from readme_ready.query import query\n",
    "from readme_ready.index import index\n",
    "from readme_ready.types import (\n",
    "    AutodocReadmeConfig,\n",
    "    AutodocRepoConfig,\n",
    "    AutodocUserConfig,\n",
    "    LLMModels,\n",
    ")\n",
    "\n",
    "# Choose model from supported models\n",
    "model = LLMModels.LLAMA2_7B_CHAT_GPTQ\n",
    "\n",
    "# Clone repository to a local path\n",
    "def git_clone(repo_url, clone_path):\n",
    "    if os.path.exists(clone_path):\n",
    "        subprocess.run(['rm', '-rf', clone_path], check=True)\n",
    "    subprocess.run(['git', 'clone', repo_url, clone_path], check=True)\n",
    "    subprocess.run(['rm', '-rf', os.path.join(clone_path,\".git\")], check=True)\n",
    "    subprocess.run(['rm', '-rf', os.path.join(clone_path,\".gitignore\")], check=True)\n",
    "\n",
    "# Parse the README.md content\n",
    "def parse_markdown(md_file_path):\n",
    "    with open(md_file_path, 'r', encoding='utf-8') as file:\n",
    "        md_content = file.read()\n",
    "\n",
    "    heading_pattern = re.compile(r'^(#+)\\s*(.*)', re.MULTILINE)\n",
    "    headings_contents = []\n",
    "    current_heading = None\n",
    "    current_content = []\n",
    "\n",
    "    for line in md_content.split('\\n'):\n",
    "        match = heading_pattern.match(line)\n",
    "        if match:\n",
    "            if current_heading is not None:\n",
    "                headings_contents.append([current_heading, ' '.join(current_content).strip()])\n",
    "            current_heading = match.group(2).strip()\n",
    "            current_content = []\n",
    "        else:\n",
    "            if line.strip():\n",
    "                current_content.append(line.strip())\n",
    "\n",
    "    if current_heading is not None:\n",
    "        headings_contents.append([current_heading, ' '.join(current_content).strip()])\n",
    "\n",
    "    df = pd.DataFrame(headings_contents, columns=['Title', 'Content'])\n",
    "    return df, md_content\n",
    "\n",
    "def clean_emoji(tx):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\", flags=re.UNICODE\n",
    "    )\n",
    "\n",
    "    return emoji_pattern.sub(r'', tx)\n",
    "\n",
    "def text_cleaner(tx):\n",
    "    text = re.sub(r\"won\\'t\", \"would not\", tx)\n",
    "    text = re.sub(r\"im\", \"i am\", tx)\n",
    "    text = re.sub(r\"Im\", \"I am\", tx)\n",
    "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "    text = re.sub(r\"don\\'t\", \"do not\", text)\n",
    "    text = re.sub(r\"shouldn\\'t\", \"should not\", text)\n",
    "    text = re.sub(r\"needn\\'t\", \"need not\", text)\n",
    "    text = re.sub(r\"hasn\\'t\", \"has not\", text)\n",
    "    text = re.sub(r\"haven\\'t\", \"have not\", text)\n",
    "    text = re.sub(r\"weren\\'t\", \"were not\", text)\n",
    "    text = re.sub(r\"mightn\\'t\", \"might not\", text)\n",
    "    text = re.sub(r\"didn\\'t\", \"did not\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    # text = re.sub('https?://\\S+|www\\.\\S+', ' ', text)\n",
    "    text = re.sub(r'https?://[^\\s\\\")]+', '', text)\n",
    "    text = re.sub(r'http?://[^\\s\\\")]+', '', text)\n",
    "    text = re.sub(r'http%3A%2F%2F[^\\s\\\")]+', '', text)\n",
    "    text = re.sub(r'https%3A%2F%2F[^\\s\\\")]+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\!\\?\\.\\@]',' ' , text)\n",
    "    text = re.sub(r'[!]+' , '!' , text)\n",
    "    text = re.sub(r'[?]+' , '?' , text)\n",
    "    text = re.sub(r'[.]+' , '.' , text)\n",
    "    text = re.sub(r'[@]+' , '@' , text)\n",
    "    text = re.sub(r'unk' , '<UNK>' , text)\n",
    "    # text = re.sub('\\n', '<NL>', text)\n",
    "    # text = re.sub('\\t', '<TAB>', text)\n",
    "    # text = re.sub(r'\\s+', '<SP>', text)\n",
    "    # text = re.sub(r'(<img[^>]*\\bsrc=\")[^\"]*(\")', '<img src=<IMG_SRC>', text)\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[ ]+' , ' ' , text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def generate_readme(name, url, repo_root, output, headings):\n",
    "    repo_config = AutodocRepoConfig (\n",
    "        name = name,\n",
    "        root = repo_root,\n",
    "        repository_url = url,\n",
    "        output = output,\n",
    "        llms = [model],\n",
    "        peft_model_path = None, # Set path to PEFT model\n",
    "        ignore = [\n",
    "            \".*\",\n",
    "            \"*package-lock.json\",\n",
    "            \"*package.json\",\n",
    "            \"node_modules\",\n",
    "            \"*dist*\",\n",
    "            \"*build*\",\n",
    "            \"*test*\",\n",
    "            \"*.svg\",\n",
    "            \"*.md\",\n",
    "            \"*.mdx\",\n",
    "            \"*.toml\"\n",
    "        ],\n",
    "        file_prompt = \"\",\n",
    "        folder_prompt = \"\",\n",
    "        chat_prompt = \"\",\n",
    "        content_type = \"docs\",\n",
    "        target_audience = \"smart developer\",\n",
    "        link_hosted = True,\n",
    "        priority = None,\n",
    "        max_concurrent_calls = 50,\n",
    "        add_questions = False,\n",
    "        device = \"auto\",\n",
    "    )\n",
    "\n",
    "    user_config = AutodocUserConfig(\n",
    "        llms = [model]\n",
    "    )\n",
    "\n",
    "    readme_config = AutodocReadmeConfig(\n",
    "        headings = headings\n",
    "    )\n",
    "\n",
    "    index.index(repo_config)\n",
    "    query.generate_readme(repo_config, user_config, readme_config)\n",
    "\n",
    "def get_score(url, readme_df, readme_content, generated_readme_df, generated_readme_content):\n",
    "    scores = {'repo': url}\n",
    "    # readme_df[\"Content\"] = readme_df[\"Content\"].apply(text_cleaner)\n",
    "    # readme_df[\"Content\"] = readme_df[\"Content\"].apply(clean_emoji)\n",
    "    # generated_readme_df[\"Content\"] = generated_readme_df[\"Content\"].apply(text_cleaner)\n",
    "    # generated_readme_df[\"Content\"] = generated_readme_df[\"Content\"].apply(clean_emoji)\n",
    "\n",
    "    # combined_df = pd.merge(readme_df, generated_readme_df, on=\"Title\", suffixes=('_target', '_pred'))\n",
    "\n",
    "    # pred = \"\\n\".join(combined_df[\"Content_pred\"].tolist())\n",
    "    # target = \"\\n\".join(combined_df[\"Content_target\"].tolist())\n",
    "\n",
    "    pred = generated_readme_content\n",
    "    target = readme_content\n",
    "\n",
    "    pred = re.sub(r' +', ' ', pred)\n",
    "    target = re.sub(r' +', ' ', target)\n",
    "    P, R, F1 = bert_score.score([pred], [target], lang='en', model_type='roberta-large', verbose=True)\n",
    "    print(P,R,F1)\n",
    "\n",
    "    scores['P'] = P.mean().item()\n",
    "    scores['R'] = R.mean().item()\n",
    "    scores['F1'] = F1.mean().item()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model.value)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    def calculate_bleu(reference, candidate):\n",
    "        reference_tokens = tokenizer.tokenize(reference)\n",
    "        candidate_tokens = tokenizer.tokenize(candidate)\n",
    "        smoothie = SmoothingFunction().method4\n",
    "        return sentence_bleu([reference_tokens], candidate_tokens, smoothing_function=smoothie)\n",
    "\n",
    "    bleu_score = calculate_bleu(target, pred)\n",
    "    print(bleu_score)\n",
    "    scores['bleu'] = bleu_score\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repos = [\n",
    "    \"https://github.com/allenai/allennlp\",\n",
    "    \"https://github.com/wting/autojump\",\n",
    "    \"https://github.com/deezer/spleeter\",\n",
    "    \"https://github.com/ddbourgin/numpy-ml\",\n",
    "    \"https://github.com/eth-siplab/TouchPose\"\n",
    "]\n",
    "scores = []\n",
    "for repo in repos:\n",
    "    name = repo.split(\"/\")[-1]\n",
    "    url = repo\n",
    "    repo_root = f\"./{name}\"\n",
    "    output = f\"./output/{name}\"\n",
    "    \n",
    "    git_clone(repo, f\"./{name}\")\n",
    "    \n",
    "    readme_df, readme_content = parse_markdown(f\"{repo_root}/README.md\")\n",
    "    headings = \",\".join(readme_df[\"Title\"].tolist())\n",
    "    \n",
    "    generate_readme(name, url, repo_root, output, headings)\n",
    "    generated_readme_df, generated_readme_content = parse_markdown(f\"{output}/docs/data/README_{model.name}.md\")\n",
    "\n",
    "    score = get_score(url, readme_df, readme_content, generated_readme_df, generated_readme_content)\n",
    "    scores.append(score)\n",
    "    subprocess.run(['rm', '-rf', repo_root], check=True)\n",
    "\n",
    "scores_df = pd.DataFrame(scores, index=None)\n",
    "scores_df.to_csv(f\"./scores_{model.name}.csv\", index=False)\n",
    "print(f\"Result saved to ./scores_{model.name}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
